---
# This script will demonstrate how to analyze the waterfowl data collected as 
# part of the LongTerm Ecological Monitoring Initiative

# Notice that this example only has 2 years of data. We simulate a third year to illustrate
# how to do a trend analysis.
#
#
# Only one study area at time can only be analyzed with a script. 
#
# This was programmed by Carl James Schwarz, Statistics and Actuarial Science, SFU
# cschwarz@stat.sfu.ca
#
# 2022-12-02 Convert to quatro
# 2017-02-28 First Edition

# Summary of Protocol
#    Establish observation points at enough sites 
#    to provide views of the entire survey area. … 
#
#    Choose a date after which migrants are mostly gone. 
#    Count on 3 separate occasions during a 2-3 week period and keep highest count.

title: "`r paste0('Waterfowl - LTEM - ',params$STUDY_AREA_NAME)`" 
date: today
date-format: "YYYY-MM-DD"
execute: 
  error: true
format: 
  html:
    toc: true
    number-sections: true
    self-contained: true
  pdf:
    toc: true
    number-sections: true
  docx:
    toc: true
    number-sections: true

params:
   STUDY_AREA_NAME: "Yellow Point"
#  STUDY_AREA_NAME: "Alice Lake Provincial Park"
---

```{r}
#| echo: false
#| warning: false
#| message: false
#| include: false

# load libraries
library(car)       # for testing for autocorrelation (2 libraries needed - see dwtest)
library(flextable) # for tables that look nice
library(ggfortify) # for residual and other diagnostic plot
library(ggplot2)   # for plotting
library(insight)   # for formatting p-values
library(lmtest)    # for testing for autocorrelation
library(lubridate) # date conversions
library(plyr)      # for group processing
library(readxl)    # for opening the Excel spreadsheets and reading off them
library(lmerTest)  # for the linear mixed modelling
library(stringr)   # string handling (like case conversion)

# Load some common functions
source("../2022-CommonFiles/common.functions.R")
source("../2022-CommonFiles/read.LTEM.R")
```


# Summary of Waterfowl LTEM protocol

## Basic protocol

As taken from the protocol document:

> “Establish observation points at enough sites to provide views of the entire survey area. … 
> 
> Choose a date after which migrants are mostly gone. Count on 3 separate occasions during a 2-3 week period and keep > highest count.”

The data collected under this protocol at each survey consists of the following variables:

-	Species. The species of waterfowl counted.
-	Sex. The sex of the species. 
-	Count. The number of birds of this species and sex. Choose a date after which migrants are mostly gone 


## Cautions about the protocol.

### Don’t use 0 to indicate a missing value.

If no species were seen during a visit, this is indicated by 
the species code being set to missing and the count field being set to 0. 
This needs to be done consistently over time.

If no visit was made to a station in a year, then there are no records in the database. 

### Classifying by season 

The protocol is silent about seasonal sampling, e.g., spring vs. fall. 
There is some issue on how to properly pool information that is close 
together but crosses year boundaries. 
For example, surveys may be done in late December of 2013 and early January of 2014. 
If the maximum counts over multiple visits in the winter are collected, 
visits on these two months need to be “combined” even though they are in different calendar years.

For this analysis we classified by month (November, December, January, February) 
to avoid this problem



## Database structure

The relevant fields from the database are:

-	*SAMPLE_LABEL*. The label for the location where data was located.
-	*Date*. The date the data was collected. The *Year* is extracted from this date.
-	*Species*. What species were seen
-	*Count*. Count of the number of birds of each species.


# Reading and checking the data

```{r}
#| echo: false
#| error: true
#| include: false


# Get the data base information and any corrections here
data.extract <- read.LTEM.data(study.type="Waterfowl",
                                         site.names=params$STUDY_AREA_NAME)
```

```{r}
#| echo: false
if(nrow(data.extract$user.data)==0){
    # no data extracted
    cat("\n\n\n*** ERROR *** No data extracted. Check your STUDY_AREA_NAME in the yaml \n")
    knitr::knit_exit()
    #stop()
}
```

The following surveys were found:

```{r}
#| echo: false
cat("Surveys with the data \n")
data.extract$projects[,c("SPI_PROJECT_ID","SURVEY_ID","START_DATE","STUDY_AREA_NAME")]
```


```{r}
#| echo: false
waterfowl.df <- data.extract$user.data
```


The following data editing was performed

## Variables names corrected for *R*

Variable names in *R* must start with a letter and contain letters or numbers or underscores.
Blanks in variable names are not normally allowed, nor are special characters such as %.
These are normally replaced by periods (".") in the variable name.

```{r}
#| echo: false

#------------ Data Editing -----------
# fix up variable names in the data.frame.
# Variable names in R must start with a letter and contain letters or number or _. 
# Blanks in variable names are not normally allowed. Blanks will be replaced by . (period)
cat("\nOriginal variable names in data frame\n")
names(waterfowl.df)

names(waterfowl.df) <- make.names(names(waterfowl.df))

cat("\nCorrected variable names of data frame\n")
names(waterfowl.df)
```

## Number of records by year

```{r}
#| echo: false
#| 

cat("\n\nThe number of records by year are \n")
xtabs(~STUDY_AREA_NAME+Year, data=waterfowl.df, exclude=NULL, na.action=na.pass)  # check the date formats. 

if(length(unique(waterfowl.df$Year))<3){
    # insufficient data extracted
    cat("\n\n\n*** ERROR *** Less than 3 years of data. No analysis possible \n")
    knitr::knit_exit()
    #stop()
}


```

## Checking Study Area Name

The Study Area Name should be recorded consistently across years, otherwise 
it may indicate that different sites are being studies. The study area name
is converted to Title Case.

The list of Study Area Names by year in the data is:

```{r}
#| echo=FALSE

# Check that the Study Area Name is the same across all years
# Look at the output from the xtabs() to see if there are multiple spellings 
# of the same STUDY_AREA_NAME.

# We will convert the STUDY_AREA_NAME to Proper Case.
waterfowl.df$STUDY_AREA_NAME <- stringr::str_to_title(waterfowl.df$STUDY_AREA_NAME)
xtabs(~STUDY_AREA_NAME+Year, data=waterfowl.df, exclude=NULL, na.action=na.pass)

if(!all(grepl(gsub(" ","",params$STUDY_AREA_NAME), gsub(" ","",waterfowl.df$STUDY_AREA_NAME), ignore.case=TRUE))){
  cat("*** ERROR *** STUDY_AREA_NAMEs are not consistent\n")
  cat("A tabulation of STUDY_AREA_NAMEs in datasets is \n")
  xtabs(~STUDY_AREA_NAME, data=waterfowl.df, exclude=NULL, na.action=na.pass)
  cat("Requested study area was ", params$STUDY_AREA_NAME,  "\n")
  knitr::knit_exit()
}

if(length(unique(waterfowl.df$STUDY_AREA_NAME))>1){
   cat("*** ERROR *** More than one study area found \n")
   cat("\n\nThe number of records by year are \n")
   xtabs(~STUDY_AREA_NAME+Year, data=waterfowl.df, exclude=NULL, na.action=na.pass)  # check the date formats. 
   knitr::knit_exit()
}

```

## Checking SAMPLE Location coded

While these are not used in the analysis, these labels should be consistent over time:

```{r}
#| echo: false
# Check the Species code to make sure that all the same
# This isn't used anywhere in the analysis but is useful to know
xtabs(~STUDY_AREA_NAME+SAMPLE_LABEL, data=waterfowl.df, exclude=NULL, na.action=na.pass)
xtabs(~SAMPLE_LABEL+Year, data=waterfowl.df, exclude=NULL, na.action=na.pass)
```


## Checking species code

The species code should be the same across the years. 
**The following table has the total number of each species seen in each combination
of location and year.**

```{r}
#| echo: false
# Check the Species code to make sure that all the same
# This isn't used anywhere in the analysis but is useful to know
addmargins(xtabs(COUNT~SPECIES_CODE+Year, data=waterfowl.df, exclude=NULL, na.action=na.pass),1)

```


```{r}
#| echo: false

# Get the file prefix
file.prefix <- make.names(waterfowl.df$STUDY_AREA_NAME[1])
file.prefix <- gsub(".", '-', file.prefix, fixed=TRUE) # convert . to 
if(!dir.exists("PLots"))dir.create("Plots")
file.prefix <- file.path("Plots", file.prefix)
```



# Single Site Analysis

Date for the `r params$STUDY_AREA_NAME` are available from `r min(waterfowl.df$Year, na.rm=TRUE)` to 
`r max(waterfowl.df$Year, na.rm=TRUE)`. 

This design has multiple stations that are repeatedly measured over time 
with multiple measurements at each station
Please refer to the Fitting Trends with Complex Study Designs document in the 
CommonFile directory for information on fitting trends with complex study designs. 

All analyses were done using the R (R Core Team, 2022)  analysis system. 
All plots are also saved as separate *png files for inclusion into other reports.

```{r}
#| echo: false
#----------------------------------------------------------------------------------------
#----------------------------------------------------------------------------------------
#----------------------------------------------------------------------------------------
#----------------------------------------------------------------------------------------
#  Analysis of the number of maximum (total) count by month.
```


## Maximum counts

The data is first summarized to the date level by summing the count over the different species
for each date-sample station combination. 
This reduces the data to one measurement per date per site/year. 

A preliminary plot of the total count is found in @fig-wf-prelim.

```{r}
#| echo: false
#| fig-cap: "Summary plot of the data. Data is plotted on the logarithmic scale because of the wide range of values. Because of potential zeros in the data, 0.5 was added to all points only for plotting purposes."
#| label: fig-wf-prelim
#| warning: false
#| message: false

# First aggregate the data by date over all species
count.date <- plyr::ddply(waterfowl.df, c("STUDY_AREA_NAME","Year","Date","SAMPLE_LABEL"), 
                          plyr::summarize,
                          total.count=sum(COUNT, na.rm=TRUE))

# Make a preliminary plot of total count by date

prelim.plot <- ggplot(data=count.date, aes(x=Date, y=log(total.count+.5), color=SAMPLE_LABEL, linetype=SAMPLE_LABEL))+
   ggtitle("Waterfowl (total) count data")+
   geom_point(position=position_dodge(width=.05))+
   geom_line( position=position_dodge(width=.05))+
   facet_wrap(~STUDY_AREA_NAME, ncol=1)
prelim.plot 
ggsave(plot=prelim.plot, 
       file=paste(file.prefix,'-plot-prelim.png',sep=""),
       h=6, w=6, units="in",dpi=300)
```

Notice that there may be evidence of station effect, where 
for example, the number of birds at certain stations are generally higher than at the other stations 
because of local station-specific conditions (e.g. better habitat).

Next the maximum count in each month in each station (but only November, December, January, and February) 
is found. The maximum counts are:

```{r}
#| echo: false
#| warning: false
#| message: false

# Find the maximum count each month (Nov/Dec/Jan/Feb only)
count.date$Month  <- lubridate::month(count.date$Date)
count.max <- plyr::ddply(count.date, c("STUDY_AREA_NAME","SAMPLE_LABEL","Year","Month"), 
                         plyr::summarize, 
                         max.count=max(total.count, na.rm=TRUE))
count.max$Year.Month <- count.max$Year+((count.max$Month-.5)/12)     # this is the approximate midpoint of each month
temp <- count.max
temp$Year.Month <- NULL
temp
```

When the maximum counts are restricted to November, December, January, and February, the
data are now:

```{r}
#| echo: false
#| warning: false
#| message: false

count.max <- count.max[ count.max$Month %in% c(1:2,11:12),]  # only retain N/D/J/F data
temp <- count.max
temp$Year.Month <- NULL
temp
```


```{r}
#| echo: false
#| warning: false
#| message: false
if(nrow(count.max)==0){
   cat("*** ERROR *** No data left to analyze when restricting data to J/F/N/D months \n")
   knitr::knit_exit()
}
```


These are plotted in @fig-wf-prelim2. 
As noted earlier, a better measure may be the median count rather than the mean count. 
The analysis would proceed in a similar fashion.

```{r}
#| echo: false
#| fig-cap: "Plot of the maximum count in each station in November through February. Data is plotted on the logarithmic scale because of the wide range of values. Because of potential zeros in the data, 0.5 was added to all points only for plotting purposes."
#| label: fig-wf-prelim2
#| warning: false
#| message: false

# Make a preliminary plot of the maximum 
prelim.max.plot <- ggplot(data=count.max, aes(x=Year.Month, y=log(max.count+.5), color=SAMPLE_LABEL, 
                                              shape=as.factor(Month), linetype=SAMPLE_LABEL))+
  ggtitle("Waterfowl (max) count in month")+
  geom_point()+
  geom_line()+
  scale_x_continuous(breaks=integer_breaks())+
  facet_wrap(~STUDY_AREA_NAME, ncol=1)+
  scale_shape_discrete(name="Month")
prelim.max.plot 
ggsave(plot=prelim.max.plot, 
       file=paste(file.prefix,'-plot-prelim-max.png',sep=""),
       h=6, w=6, units="in",dpi=300)



```

Again there may be evidence of a consistent station effect and/or a consistent month effect.


Because this is count data, so a linear mixed model is fit to the logarithm of the maximum call 
in each month at each transect in each year. The model is:
 $$log(MaxCount) = \mathit{Year} + \mathit{MonthF}+ \mathit{StationF(R)} +\mathit{YearF(R)}$$
where 

- $log(MaxCount)$ is logarithm of the maximum count at that station in that month in that year; 
- $\mathit{StationF}$ represents the station effect; 
- $\mathit{YearF}$ represents the year-specific effects (process error), and 
- $\mathit{Year}$ represents the calendar year trend over time. 

The $\mathit{StationF}$ term allows for the fact that station-specific conditions may tend to 
affect the counts on this station consistently over time. 
The $\mathit{YearF}$ term represent the year-specific effects (process error) 
caused by environmental factors (e.g., a warmer than normal year may elicit more visits from waterfowl).

This model implicitly assumes that the trend in Nov, Dec, Jan and February 
are the same but with a different intercept (i.e. parallel lines on the logarithmic scale).

Model fit on the logarithmic scale assume that effects are multiplicative over time, 
so that the when the actual fit is done on the logarithmic scale, 
the trends are linear. 
For example, a trend may assume that there is constant 5% change over time 
rather than a fixed 1 unit change per year. 
Some caution is needed if any of the values are 0 as log(0) is not defined. 
In these cases, a small constant (typically ½ of the smallest positive value in the dataset) 
is added to all values before the analysis proceeds.

This model is fit using the *lmer()* function in the lmerTest package 
(Kuznetsova, et al. 2016) 


```{r}
#| echo: false
#| 
#----------------------------------------------------------------------------------------
#  Analysis of the maximum count by month

# This is a regression analysis with Year as the trend variable and Month as a seasonal effect.
# We need to account for the same stations being measured over time and for year specific effects (process error).
# Because of the small counts, we will try a Poisson model (allowing for possible overdispersion
# It would be nice to fit a generalized linear mixed model, but there are convergence problems

count.max.pvalue   <- NA # in case model fails
count.max.slope    <- NA
count.max.slope.se <- NA


count.max$MonthF                <- factor(count.max$Month)
count.max$SAMPLE_LABELF <- factor(count.max$SAMPLE_LABEL)
count.max$YearF                 <- factor(count.max$Year)

count.max.lmer <- lmerTest::lmer(log(max.count+.5) ~ Year + MonthF +(1|SAMPLE_LABELF) +(1|YearF), data=count.max)

anova(count.max.lmer, ddf="Kenward-Roger")
summary(count.max.lmer)
VarCorr(count.max.lmer)

count.max.pvalue <- anova(count.max.lmer, ddf="Kenward-Roger")[1,"Pr(>F)"]
count.max.slope  <- fixef(count.max.lmer)[2]
count.max.slope.se <- sqrt(diag(vcov(count.max.lmer)))[2]
```



```{r}
#| echo: false
#| fig-cap: "Summary plot of the trend in maximum birds counts. Because the model was fit on the logarithmic scale, the fitted trend line is not a straight line but curved."
#| label: fig-max-trend
#| warning: false
#| message: false

# extract a table of the slopes
count.slopes <- data.frame(
       STUDY_AREA_NAME =count.max$STUDY_AREA_NAME[1],
       slope           = fixef(count.max.lmer)["Year"],
       slope.se        = summary(count.max.lmer)$coefficients["Year","Pr(>|t|)"],
       p.value         = summary(count.max.lmer)$coefficients[row.names(summary(count.max.lmer)$coefficients)=="Year"  ,"Pr(>|t|)"], 
       #r2             = summary(count.max.lmer)$r.squared,  # not defined for mixed effect models
       stringsAsFactors=FALSE)
count.slopes


# compute the fitted values from the model
# The model was run on the log(average count), so we need to back transform
count.fitted <- expand.grid(
                 STUDY_AREA_NAME=count.max$STUDY_AREA_NAME[1],
                 Year=seq(min(count.max$Year, na.rm=TRUE),1+max(count.max$Year, na.rm=TRUE), .1),
                 MonthF=factor(c(1:2,11:12)),
                 stringsAsFactors=FALSE)
count.fitted$pred.mean <- exp(predict(count.max.lmer, newdata=count.fitted,type="response", re.form=~0))
#head(count.fitted)



# Plot with trend line with a separate plot for each StudyArea
count.plot.summary <- ggplot2::ggplot(data=count.max,
                                    aes(x=Year.Month, y=max.count))+
   ggtitle("Waterfowl counts ")+
   ylab("Maximum Waterfowl Count in month")+
   geom_point(size=3, aes(color=MonthF))+
   geom_line(data=count.fitted, aes(x=Year,y=pred.mean, color=MonthF))+
   facet_wrap(~STUDY_AREA_NAME, ncol=1, scales="free" )+
   scale_x_continuous(breaks=integer_breaks())+
   geom_text(data=count.slopes, aes(x=min(count.max$Year, na.rm=TRUE), y=max(count.max$max.count, na.rm=TRUE)), 
             label=paste("Slope (on log scale) : ",round(count.slopes$slope,2), 
                         " ( SE "  ,round(count.slopes$slope.se,2),")",
                         " p :"    ,round(count.slopes$p.value,3)),
                         hjust="left")+
   scale_color_discrete(name="Month")
count.plot.summary
ggsave(plot=count.plot.summary, 
       file=paste(file.prefix,'-count-plot-summary.png',sep=""),
       h=6, w=6, units="in", dpi=300)


exp.count.max.slope = exp(count.max.slope)
```

@fig-max-trend shows a summary plot, along with estimates of the slope, its standard error,
and the p-value of the hypothesis of no trend. With 
`r length(unique(waterfowl.df$Year))` years of data, 
the estimated slope is 
`r try(round(count.max.slope,3), silent=TRUE)` 
(SE `r try(round(count.max.slope.se,3), silent=TRUE)`) /year 
(`r try(insight::format_p(count.max.pvalue), silent=TRUE)`).

The reported common slope is on the logarithmic scale.
This corresponds to an approximate exp(`r try(round(count.max.slope,3),silent=TRUE)`)=
`r try(round(exp.count.max.slope,2), silent=TRUE)`x 
multiplicative change/year, i.e. the mean maximum count in year $t+1$ is about 
`r try(round(exp.count.max.slope,2), silent=TRUE)`x the mean maximum count in year $t$.
Because the analysis is done on the logarithmic scale, the fitted trend line will 
look non-linear on the original (non-transformed) scale.

```{r}
#| echo: false
#| fig-cap: "Model fit diagnostic plots from trend in maximum count"
#| label: fig-count-max-resid
#| warning: false
#| message: false


# Look at the residual plots and save them to the directory
count.max.diag.plot <- sf.autoplot.lmer(count.max.lmer)  # residual and other diagnostic plots
plot(count.max.diag.plot)
ggsave(#plot=count.max.diag.plot, #bug in ggsave with ggmultiplots that you don't specify object
       file=paste(file.prefix,"-count-max-residual-plot.png",sep=""),
       h=6, w=6, units="in", dpi=300)
```


Residual plots are presented in (@fig-count-max-resid).
In the upper left corner is a plot of residuals vs. 
the fitted values. A good plot will show a random scatter around 0. 
Any large deviations from 0 should be investigated as potential outliers. 
In the upper right is a normal probability plot. Points should be close to the dashed reference line. 
Fortunately, the analysis is fairly robust against non-normality so only extreme departures are worrisome. 
The bottom left plot shows that the year specific effects are very small (the dots are all close to 0).
The bottom right plot shows that the station effects with some stations tending to have higher counts than other stations. 



```{r}
#| echo: false
#| warning: false
#| message: false

# Check for autocorrelation 
count.max$resid <- log(count.max$max.count+.5) - predict(count.max.lmer, newdata=count.max, re.form=~0)
mean.resid <- plyr::ddply(count.max, "Year", summarize, mean.resid=mean(resid))
resid.fit <- lm( mean.resid ~ 1, data=mean.resid)
count.max.dwres1 <- car::durbinWatsonTest(resid.fit)
#dwres1
count.max.dwres2 <- lmtest::dwtest(resid.fit)
#dwres2

```

Whenever an analysis of a trend over time is conducted, the analysis 
should test and adjust for autocorrelation. 
Autocorrelation usually isn’t a problem (and likely cannot be detected) unless you have 10+ years of data. 
The test for autocorrelation commonly used is the Durbin-Watson test and we find
(`r try(insight::format_p(count.max.dwres1$p), silent=TRUE)` for the test of no autocorrelation.


The analysis of the median count per month would follow the same steps as shown above 
and the R code is easy to modify. 
Similarly, this analysis was conducted at the total count level (over all species) but 
could be done for individual species. 
One potential problem is that in some cases, species information is only recorded at the 
Genus or higher level. 
In these case, this data will have to discarded when the analysis is done at the species level, 
but then you are making an implicit assumption that recording at the Genus level 
happens at random and is unrelated to the response. 
If this assumption is violated (e.g. perhaps when there are larger number of birds, 
it is too difficult to record at the individual species level) then this is not occurring at 
random and some effort must be made to “split” the genus level information among the species.

Trends over time could also occur in the diversity of the birds. 
In theory, standard diversity measures could be used and tracked over time, but 
these have a very strong assumption that all species are equally detectable by the observer. 
This is unlikely to be true. Secondly, the actual counts are quite small, 
and diversity measures that rely on actual counts (e.g. Simpson’s diversity) 
will not perform well. For this reason, I do not recommend an analysis on the diversity of the observations.


# Power analysis

A power/sample size analysis was conducted to determine the number of years of sampling needed to detect
changes over time. The steps in the power/sample size analysis are:

- Compute a single number summarizing the response at this site in each year. The mean number of calls is currently recorded by transect and the average
over the transects will be computed.
- Analyze the log(mean response) using a simple linear regression. This will give an estimate of the combined year-specific and sampling variation
around regression line (overall sd).
- Use the overall SD to estimate power and sample size requirements.

Here are the overall means for each year for each response:

```{r}
#| echo: false
#| warning: false
#| message: false
#| error: true

# get the mean values of 1 number per site per year. Notice that since the counts are taken in N/D/J/F, we need to use a "winter year" rather
# than a calendar year to group counts properly

temp <- count.max
temp$oldYear <- temp$Year
temp$Month
temp$Year[ temp$Month %in% c(1,2)] <- temp$Year[ temp$Month %in% c(1,2)] -1

count.mean1 <- plyr::ddply(count.max, c("STUDY_AREA_NAME","Year"), plyr::summarize,
                                mean.max=mean(max.count, na.rm=TRUE))
count.mean1$Mean.response <- count.mean1$mean.max
count.mean1$Response      <- "Mean max N/D/J/F"

all.resp <- plyr::rbind.fill(count.mean1)
all.resp

```

The above data needs to be checked if there are any suspicious values.  

We use the *lm()* function to fit linear regression over time on the log(scale) and obtain the combined year-specific effect (process error) 
plus sampling variation standard deviation (@tbl-resid-sd).

```{r}
#| echo: false
#| warning: false
#| message: false
#| error: true
#| tbl-cap: "Estimated residual (process + sampling) standard deviation"
#| label: tbl-resid-sd

# if any of the values are zero, add a small offset
all.resp <- plyr::ddply(all.resp, "Response", function(x){
     offset <- min(x$Mean.response[x$Mean.response>0]) *.5
     x$Mean.response <- x$Mean.response + offset
     x
})

# fit a regression line on the log(scale) and get the residual sd
residual.sd <- plyr::ddply(all.resp, "Response", function(x){
    fit <- lm(log(Mean.response) ~ Year, data=x)
    sd  <- summary(fit)$sigma
    data.frame(sd=sd)
})

ft <- flextable(residual.sd)
ft <- width(ft, j=1, width=2)
ft <- width(ft, j=2, width=2)
ft <- colformat_double(ft, j=2, digits=4)
ft <- set_header_labels(ft, values = list(sd="Process + sampling SD"))
 
ft
```

This is then used to estimate the power to detect various proportional changes over time (@fig-power).

```{r}
#| echo: false
#| warning: false
#| message: false
#| error: true
#| fig-cap: "Estimated power to detect proportional changes over time"
#| label: fig-power

# remove any missing values for resid.sd
residual.sd <- residual.sd[!is.na(residual.sd$sd),]

# estimate power at changes of 0 to .10/year with sample sizes of 5 to 20 years for each response
scenarios <- expand.grid(n.years=seq(5,20,1),
                         slope  =seq(0, .1, .02))
power.detect <- plyr::ddply(residual.sd, "Response", function(x, scenarios){
   
   power <- plyr::adply(scenarios,1, function(scenario,x){
      #browser()
      power <- slr.power.stroup(Trend=scenario$slope,
                                Xvalues=1:scenario$n.years,
                                Process.SD=x$sd,
                                Sampling.SD=0)
      power
   },x=x)
}, scenarios=scenarios)

ggplot(data=power.detect, aes(x=n.years, y=power.2s, color=as.factor(slope)))+
   ggtitle("Estimated power", subtitle="alpha=0.05")+
   geom_point()+
   geom_line()+
   facet_wrap(~Response, ncol=1)+
   ylab("Power")+xlab("Number of years in the study")+
   scale_color_discrete(name="Proportional\nyearly\nslope")+
   xlim(0,NA)+ylim(0,1)+
   geom_hline(yintercept=0.8)


```
 
The proportional yearly slope indicates the effect size of interest. For example, a value
of .02 would indicated a 2% change/year in the mean response.
 
In cases of high variability, the power is uniformly low 
to detect the yearly proportion change over time. 
In cases of low variability power is uniformly very high
to detect the yearly proportion change over time.


# Summary

Some caution is required to ensure that all stations are visited equally often in a year. 
In this balanced design, it is straightforward to compute statistics over all measurements 
of a station in a month and all stations in a year have the same number of visits. 
It is possible to modify the analysis if only some stations are visited on a particular 
date with an unequal number of visits to a station in a year. 
A simple way to deal with unbalance would be to delete some of the observations, but better methods are available.

The protocol assumes that the observation points allow the observer to see all 
the waterfowl on the wetland between them so differential coverage over time may not be a problem.


# References

Kuznetsova A, Brockhoff PB, Christensen RHB (2017). 
lmerTest Package: Tests in Linear Mixed Effects Models.
Journal of Statistical Software, 82, 1-26. 
doi:10.18637/jss.v082.i13
 
R Core Team (2022). R: A language and environment for statistical computing. 
R Foundation for Statistical Computing, Vienna, Austria. 
https://www.R-project.org/.


