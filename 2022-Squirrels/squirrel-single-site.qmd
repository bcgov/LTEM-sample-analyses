---
# This script will demonstrate how to analyze the Squirrel data collected as 
# part of the LongTerm Ecological Monitoring Initiative
#
# Only one study area can be analyzed with a script. 
#
# This was programmed by Carl James Schwarz, Statistics and Actuarial Science, SFU
# cschwarz@stat.sfu.ca
#
# 2022-11-20 Revised edition 
#    - changed to using Quarto to integrate the MSWord and R code together into one document
# 2017-02-28 First Edition

# Summary of Protocol
#    Red squirrels regularly emit an audible rattle, 
#    especially when their territories are invaded. 
#    This protocol involves walking a transect (a section of a trail) 
#    and recording the location of rattles heard along the way. 
#
#    Locate as many transects in a given area as possible (up to 5). 
#    Sample them annually but in a different order each year. 
#    Sampling involves walking a defined segment of trail and recording 
#    squirrel rattles or chattering.
#
title: "`r paste0('Red Squirels - LTEM - ',params$STUDY_AREA_NAME)`" 
date: today
date-format: "YYYY-MM-DD"
execute: 
  error: true
format: 
  html:
    toc: true
    number-sections: true
    self-contained: true
  pdf:
    toc: true
    number-sections: true
  docx:
    toc: true
    number-sections: true

params:
  STUDY_AREA_NAME: "Bowser"
---

```{r}
#| echo: false
#| warning: false
#| message: false
#| include: false

# load libraries
library(broom.mixed)# for handling output from lmerTest
library(car)       # for testing for autocorrelation (2 libraries needed - see dwtest)
library(flextable) # for tables that look nice
library(ggfortify) # for residual and other diagnostic plot
library(ggplot2)   # for plotting
library(insight)   # for formatting p-values
library(lmtest)    # for testing for autocorrelation
library(lubridate) # date conversions
library(plyr)      # for group processing
library(readxl)    # for opening the Excel spreadsheets and reading off them
library(sf)
library(lmerTest)  # for the linear mixed modelling
library(stringr)   # string handling (like case conversion)

# Load some common functions
source("../2022-CommonFiles/common.functions.R")
source("../2022-CommonFiles/read.LTEM.R")
```


# Summary of Red Squirrel LTEM protocol

## Basic protocol
As taken from the protocol document:

> “Red squirrels regularly emit an audible rattle, especially when their territories 
> are invaded. This protocol involves walking a transect (a section of a trail) 
> and recording the location of rattles heard along the way. 

For this protocol, locate as many transects in a given area as possible (up to 5). 
Sample them annually but in a different order each year. 
Sampling involves walking a defined segment of trail and recording squirrel rattles or chattering.

The data collected under this protocol consists of the.

-	Detection Type. Was a call heard, a visual observation made or other evidence of a squirrel. 
Only calls will be used in this analysis
-	Distance along the transect. The location of the observer along the transect when the detection was made. 
-	Distance from the transect. 
The perpendicular distance from the transect line to the observation along with the side of the transect. 



## Cautions about the protocol.

### Don’t use 0 to indicate a missing value.

If no calls were heard on a transect, how is this indicated? 
For example, in the 2013 data table, there is NO observation for 
transect 4 on 2013-09-13. 
Does this mean that the transect was not run, or was it run and no squirrels were detected? 
The Transect Information sheet in the workbook has the GPS co-ordinates of each transect, 
but does not indicate if the transect was run on each of the days. 

It will be assume that ALL transects listed in the database were run and
have 0 already imputed.


### No information on transect length available
.
There is no information about the length of each transect or 
if each transect was visited on its entirety each visit. 
It will be assumed that each transect is approximately equal 
in length and that the entire transect is visited on a visit.

### Not suitable for distance sampling to estimate density.

The current data looks very similar to captured by distance sampling 
methods where RANDOM transects are selected in the study 
area and the perpendicular distance of observations to the transect are selected. 
Distance sampling is used to estimate density. 

In this protocol, transects are not selected at random. 
Indeed, according to the protocol,

> “Find a location that is not difficult to access and where there are abundant squirrels.”

So the apparent density of squirrels may be biased upwards by the selection of transects

### Be careful to document changes in transect over time.

The protocol is silent on how to document changes in transects over time. 
For example, suppose that a transect is damaged by fire? 
How is this recorded? Suppose that a transect is abandoned and new transect is chosen. 
At the very least, the transect label should NOT be recycled over time. 

### Not clear how to group visits.

The date that the transects are visited is also recorded. 
It is assumed that all transects will be visited on the same date. 
However, in some cases, the transects are visited over a span of 2 or 3 day – 
these should presumably  be “pooled” into one visit. 
At the moment, there is no way to decide if all transects were visited on a single day, 
or if a “visit” corresponds to more than one day. 
A field should be added to the data base for the “visit”, e.g., 
if it takes several days to visit all transects, these should be either be recorded on the first date, 
or all take the same visit indicator.

## Database structure

The relevant fields extracted from the database are:

-	*Transect Label*.
-	*Date*. The date the data was collected. The Year is extracted from this date.
-	*Count*. The number of calls heard on the transect on that date.
It is assumed that 0's have been already imputed on the database.


# Reading and checking the data

The database was queried for all observations from
`r params$STUDY_AREA_NAME`.

```{r}
#| echo: false
#| include: false

# get the data from the Excel work.books.
# we put the list of work books here, including the file type (xls or xlsx).
# You can put multiple stations here because the station information is included on the raw data

# Get the data base information and any corrections here
data.extract <- read.LTEM.data(study.type="Squirrels",
                                         site.names=params$STUDY_AREA_NAME)
```


```{r}
#| echo: false
if(nrow(data.extract$user.data)==0){
    # no data extracted
    cat("\n\n\n*** ERROR *** No data extracted. Check your STUDY_AREA_NAME in the yaml \n")
    knitr::knit_exit()
    #stop()
}

```

The following surveys were found:

```{r}
#| echo: false
cat("Surveys with the data \n")
data.extract$projects[,c("SPI_PROJECT_ID","SURVEY_ID","START_DATE","STUDY_AREA_NAME")]
```

```{r}
#| echo: false

squirrel.df <- data.extract$user.data
```

The following data editing was performed

## Variables names corrected for *R*

Variable names in *R* must start with a letter and contain letters or numbers or underscores.
Blanks in variable names are not normally allowed, nor are special characters such as %.
These are normally replaced by periods (".") in the variable name.

```{r}
#| echo: false

## fix up variable names in the data.frames.
# Variable names in R must start with a letter and contain letters or number or _. 
# Blanks in variable names are not normally allowed. Blanks will be replaced by . (period)
cat("\nOriginal variable names in squirrels data file\n")
names(squirrel.df)

names(squirrel.df) <- make.names(names(squirrel.df))

cat("\nCorrected variable names of data frame\n")
names(squirrel.df)

```

## How many years of data are available?

```{r}
#| echo: false
#| error: true
#| 

cat("\n\nThe number of TRANSECTS run by year are \n")
xtabs(~STUDY_AREA_NAME+Year, data=squirrel.df, exclude=NULL, na.action=na.pass)  # check the date formats. 

if(length(unique(squirrel.df$Year))<3){
    # insufficient data extracted
    cat("\n\n\n*** ERROR *** Less than 3 years of data. No analysis possible \n")
    knitr::knit_exit()
    stop()
}
```

## Checking Study Area Name

The Study Area Name should be recorded consistently across years, otherwise 
it may indicate that different sites are being studied. The study area name
is converted to Title Case.

The list of number of transects by each combination of Study Area Names by year in the data is:

```{r}
#| echo=FALSE

# Check that the Study Area Name is the same across all years
# Look at the output from the xtabs() to see if there are multiple spellings 
# of the same STUDY_AREA_NAME.

# We will convert the STUDY_AREA_NAME to Proper Case.
squirrel.df$STUDY_AREA_NAME <- stringr::str_to_title(squirrel.df$STUDY_AREA_NAME)
xtabs(~STUDY_AREA_NAME+Year, data=squirrel.df, exclude=NULL, na.action=na.pass)

if(!all(grepl(params$STUDY_AREA_NAME, squirrel.df$STUDY_AREA_NAME, ignore.case=TRUE))){
  cat("*** ERROR *** STUDY_AREA_NAMEs are not consistent in calls sheet\n")
  cat("A tabulation of STUDY_AREA_NAMEs in datasets is \n")
  xtabs(~STUDY_AREA_NAME, data=squirrel.df, exclude=NULL, na.action=na.pass)
  cat("Requested study area was ", params$STUDY_AREA_NAME,  "\n")
  stop()
}

if(length(unique(squirrel.df$STUDY_AREA_NAME))>1){
   cat("*** ERROR *** More than one study area found in call sheet\n")
   cat("\n\nThe number of records by year are \n")
   xtabs(~STUDY_AREA_NAME+Year, data=squirell.df, exclude=NULL, na.action=na.pass)  # check the date formats. 
   stop()
}


```


## Checking species code

The species code should be the same across for all data values..
Note that in many years, a code of *NULL* was entered to indicate no squirrels of any species
were detected. **This table shows the total detections for each species.year combination over all the transects**

```{r}
#| echo: false
# Check the Species code to make sure that all the same
# This isn't used anywhere in the analysis but is useful to know
xtabs(COUNT~SPECIES_CODE+Year, data=squirrel.df, exclude=NULL, na.action=na.pass)

if(length(unique(squirrel.df$SPECIES_CODE))>1){
   cat("*** WARNING *** More than one species name found - OK if some are NULL \n")
   #stop()
}

```


## Summarized data to the Year.Transect level

We summarize the data to mean number of calls per year per transect.

```{r}
#| echo: false
# Summarize the imputed data to one number per year per transect
count.transect <- plyr::ddply(squirrel.df, c("STUDY_AREA_NAME","Year","TRANSECT"), 
                              plyr::summarize, mean.calls=mean(COUNT))
count.transect

```


```{r}
#| echo: false

# Get the file prefix
file.prefix <- make.names(squirrel.df$STUDY_AREA_NAME[1])
file.prefix <- gsub(".", '-', file.prefix, fixed=TRUE) # convert . to 
if(!dir.exists("Plots"))dir.create("Plots")
file.prefix <- file.path("Plots", file.prefix)
```


# Single Site Analysis

Date for the `r params$STUDY_AREA_NAME` are available from `r min(squirrel.df$Year, na.rm=TRUE)` to 
`r max(squirrel.df$Year, na.rm=TRUE)`. 

This design has multiple transects that are repeatedly measured over time. 
Please refer to the Fitting Trends with Complex Study Designs document in the 
CommonFile directory for information on fitting trends with complex study designs. 

All analyses were done using the R (R Core Team, 2022)  analysis system. 
All plots are also saved as separate *png files for inclusion into other reports.


## Calls.

The data is first summarized to the transect-year level by finding 
the mean number of calls on a transect over multiple visits for each individual transect. 
This reduces the data to one measurement per transect per site/year. 
It is implicitly assumed that all transects are run on all days within a 
so every transect has the same number of days of measurement. 
If transects are changed over time, that is not a problem, but transects 
should not be introduced or removed part way through a year.


A summary plot of the mean number of calls on each transect is shown in @fig-call-prelim.

```{r}
#| echo: false
#| fig-cap: "Summary plot of the data. The data is analyzed on the logarithmic scale."
#| label: fig-call-prelim
#| warning: false
#| message: false

prelim.plot <- ggplot(data=count.transect, aes(x=Year, y=log(mean.calls+.5), color=TRANSECT, shape=TRANSECT))+
   ggtitle("Squirrel count data")+
   ylab("log(Mean count+.5)")+
   geom_point(position=position_dodge(width=.1))+
   geom_line( position=position_dodge(width=.1))+
   facet_wrap(~STUDY_AREA_NAME, ncol=1)+
   scale_x_continuous(breaks=integer_breaks())
prelim.plot 
ggsave(plot=prelim.plot, 
       file=paste(file.prefix,'-plot-prelim.png', sep=""),
       h=4, w=6, units="in",dpi=300)

```


There may be evidence of a transect effect, where, for example, 
the number of calls at certain transects could generally be higher than at the 
other transects because of local transect-specific conditions (e.g. better habitat).

Because this is count data,  a linear mixed model is fit to the logarithm of the mean calls per transect.
The model is:

$$log(AvgCalls+.5) \sim \mathit{Year} + \mathit{TransectF(R)} + \mathit{YearF(R)}$$
where 

- $log(AvgCalls)$ is logarithm of the average number of calls for that transect in that year. An offset of 0.5
is used to avoid taking the logarithm of 0.
- $\mathit{TransectF(R)}$ represents the (random)transect effect; 
- $\mathit{YearF(R)}$ represents the (random) year-specific effects (process error), and 
- $\mathit{Year}$ term represents the calendar year trend over time. 

The $\mathit{TransectF}$ term allows for the fact that transect-specific conditions 
may tend to affect the counts on this transect consistently over time. 
The $\mathit{YearF}$ term represent the year-specific effects (process error) 
caused by environmental factors (e.g., a warmer than normal year may elicit more calls from squirrels).

Model fit on the logarithmic scale assume that effects are multiplicative over time, 
so that the when the actual fit is done on the logarithmic scale, 
the trends are linear. For example, a trend may assume that there is constant 
5% change over time rather than a fixed 1-unit change per year. 
Some caution is needed if any of the values are 0 as log(0) is not defined. 
In these cases, a small constant (typically ½ of the smallest positive value in the dataset) 
is added to all values before the analysis proceeds.

The model was fit using the *lmer()* function in *R*. 

```{r}
#| echo: false
#| 

count.fit.pvalue     <- NA # in case the model does not fit
count.fit.slope      <- NA
count.fit.slope.se   <- NA
count.fit.slope.anti <- NA

count.transect$YearF     <- factor(count.transect$Year)
count.transect$TRANSECTF <- factor(count.transect$TRANSECT)
count.fit <- lmerTest::lmer(log(mean.calls+.5) ~ Year + (1|TRANSECTF) + (1|YearF), data=count.transect)

cat("\n\n")
anova(count.fit, ddf="Kenward-Roger")
cat("\n\n")
summary(count.fit)

count.fit.pvalue <- anova(count.fit, ddfm="Kenward-Roger")[1,"Pr(>F)"]
count.fit.slope  <- fixef(count.fit)[2]
count.fit.slope.se <- sqrt(diag(vcov(count.fit)))[2]

count.fit.slope.anti <- exp(count.fit.slope)

```

@fig-count-trend shows a summary plot, along with estimates of the slope, its standard error,
and the p-value of the hypothesis of no trend. With 
`r length(unique(squirrel.df$Year))` years of data, 
the estimated slope on the logarithmic scale is 
`r try(round(count.fit.slope,3), silent=TRUE)` 
(SE `r try(round(count.fit.slope.se,3), silent=TRUE)`) calls/year 
(`r try(insight::format_p(count.fit.pvalue), silent=TRUE)`).

This corresponds to an approximate exp(`r try(round(count.fit.slope,3), silent=TRUE)`)=
`r try(round(count.fit.slope.anti,2), silent=TRUE)`x
multiplicative change/year, 
i.e. the mean count in year $t+1$ is about `r try(round(count.fit.slope.anti,2), silent=TRUE)`x 
the mean count in 
year $t$.
Because the analysis is done on the logarithmic scale, the fitted trend line looks non-linear 
on the original (non-transformed) scale.

```{r}
#| echo: false
#| fig-cap: "Summary plot of the trend in mean squirrel counts. Because Poisson regression operates on the logarithmic scale, the fitted trend line is not a straight line but curved."
#| label: fig-count-trend
#| warning: false
#| message: false

# extract a table of the slopes
count.slopes <- data.frame(
       STUDY_AREA_NAME = count.transect$STUDY_AREA_NAME[1],
       slope           = fixef(count.fit)["Year"],
       slope.se        = summary(count.fit)$coefficients["Year","Pr(>|t|)"],
       p.value         = summary(count.fit)$coefficients[row.names(summary(count.fit)$coefficients)=="Year"  ,"Pr(>|t|)"], 
       #r2             = summary(count.fit)$r.squared,  # not defined for mixed effect models
       stringsAsFactors=FALSE)
#count.slopes


# compute the fitted values from the model
# The model was run on the log(average count), so we need to back transform
count.fitted <- data.frame(
                 STUDY_AREA_NAME=count.transect$STUDY_AREA_NAME[1],
                 Year=seq(min(count.transect$Year, na.rm=TRUE),max(count.transect$Year, na.rm=TRUE), .1),
                 stringsAsFactors=FALSE)
count.fitted$pred.mean <- exp(predict(count.fit, newdata=count.fitted,type="response", re.form=~0))-.5 # remove the offset
#head(count.fitted)


# Make the summary plot with the estimated slope and fitted line
count.plot.summary <- ggplot2::ggplot(data=count.transect,
                                    aes(x=Year, y=mean.calls))+
   ggtitle("Squirrel count ")+
   ylab("Squirrel Count")+
   geom_point(size=3, aes(color=TRANSECT))+
   geom_line(data=count.fitted, aes(y=pred.mean))+
   facet_wrap(~STUDY_AREA_NAME, ncol=1, scales="free" )+
   scale_x_continuous(breaks=integer_breaks())+
   geom_text(data=count.slopes, aes(x=min(count.transect$Year, na.rm=TRUE), y=max(count.transect$mean.calls, na.rm=TRUE)), 
             label=paste("Slope (on log scale) : ",round(count.slopes$slope,2), 
                         " ( SE "  ,round(count.slopes$slope.se,2),")",
                         " p :"    ,round(count.slopes$p.value,3)),
                         hjust="left")
count.plot.summary
ggsave(plot=count.plot.summary, 
       file=paste(file.prefix,'-count-plot-summary.png',sep=""),
       h=6, w=6, units="in", dpi=300)

```



```{r}
#| echo: false
#| fig-cap: "Model fit diagnostic plots from trend in mean transect counts"
#| label: fig-count-resid
#| warning: false
#| message: false

# Look at the residual plots and save them to the directory
diag.plot <- sf.autoplot.lmer(count.fit)  # residual and other diagnostic plots
plot(diag.plot)
ggsave(plot=diag.plot, 
       file=paste(file.prefix,"-count-residual-plot.png",sep=""),
       h=6, w=6, units="in", dpi=300)

```

Residual plots are presented in @fig-count-resid.
With only `r length(unique(squirrel.df$Year))` years of data, 
the plots are not very informative. In the upper left corner is a plot of residuals vs. 
the fitted values. A good plot will show a random scatter around 0. 
Any large deviations from 0 should be investigated as potential outliers. 
In the upper right is a normal probability plot. Points should be close to the dashed reference line. 
Fortunately, the analysis is fairly robust against non-normality so only extreme departures are worrisome. 
Caterpillar plots attempt to show the distribution of the random effects. 
The bottom left plot shows the distribution of the transect effects. 
The bottom right plot shows the distribution of the year-specific effects (process variation). 
In this case, the estimated process variation is very small with most of points very close to 0.

It will also be possible to covariates such as mean winter temperature or degree days 
in the year to try and explain some of the variation over time using a multiple regression. 
If there is only a small number of years of data available, this may not sensible.



```{r}
#| echo: false
#| warning: false
#| message: false

# get the autocorrelation 
count.transect$resid <- log(count.transect$mean.calls+.5) - 
                        predict(count.fit, newdata=count.transect, re.form=~0)
mean.resid <- plyr::ddply(count.transect, "Year", summarize, mean.resid=mean(resid))
resid.fit <- lm( mean.resid ~ 1, data=mean.resid)
count.dwres1 <- car::durbinWatsonTest(resid.fit)
#count.dwres1
count.dwres2 <- lmtest::dwtest(resid.fit)
#count.dwres2

```

Whenever an analysis of a trend over time is conducted, the analysis 
should test and adjust for autocorrelation. 
Autocorrelation usually isn’t a problem (and likely cannot be detected) unless you have 10+ years of data. 
The test for autocorrelation commonly used is the Durbin-Watson test and the p-value is 
`r try(insight::format_p(count.dwres1$p), silent=TRUE)` 
for the test of no autocorrelation.

This model used the approximate analysis on the logarithm of the average counts per transect. 
It is possible to analyze the actual raw counts using a generalized linear mixed model – 
this was not done in this example because of the extreme smallness of the dataset. 
Once many more years are collected, this may be an alternative analysis 
that will more naturally deal with 0 counts without having to add a small constant.




# Power analysis

A power/sample size analysis was conducted to determine the number of years of sampling needed to detect
changes over time. The steps in the power/sample size analysis are:

- Compute a single number summarizing the response at this site in each year. The mean number of calls is currently recorded by transect and the average
over the transects will be computed.
- Analyze the log(mean response) using a simple linear regression. This will give an estimate of the combined year-specific and sampling variation
around regression line (overall sd).
- Use the overall SD to estimate power and sample size requirements.

Here are the overall means for each year for each response:

```{r}
#| echo: false
#| warning: false
#| message: false
#| error: true

# get the mean values of 1 number per site per year

count.mean1 <- plyr::ddply(count.transect, c("STUDY_AREA_NAME","Year"), plyr::summarize,
                                mean.calls=mean(mean.calls, na.rm=TRUE))
count.mean1$Mean.response <- count.mean1$mean.calls
count.mean1$Response      <- "Calls/transect"

all.resp <- plyr::rbind.fill(count.mean1)
all.resp

```

The above data needs to be checked if there are any suspicious values.  

We use the *lm()* function to fit linear regression over time on the log(scale) and obtain the combined year-specific effect (process error) 
plus sampling variation standard deviation (@tbl-resid-sd).

```{r}
#| echo: false
#| warning: false
#| message: false
#| error: true
#| tbl-cap: "Estimated residual (process + sampling) standard deviation"
#| label: tbl-resid-sd

# if any of the values are zero, add a small offset
all.resp <- plyr::ddply(all.resp, "Response", function(x){
     offset <- min(x$Mean.response[x$Mean.response>0]) *.5
     x$Mean.response <- x$Mean.response + offset
     x
})

# fit a regression line on the log(scale) and get the residual sd
residual.sd <- plyr::ddply(all.resp, "Response", function(x){
    fit <- lm(log(Mean.response) ~ Year, data=x)
    sd  <- summary(fit)$sigma
    data.frame(sd=sd)
})

ft <- flextable(residual.sd)
ft <- width(ft, j=1, width=2)
ft <- width(ft, j=2, width=2)
ft <- colformat_double(ft, j=2, digits=4)
ft <- set_header_labels(ft, values = list(sd="Process + sampling SD"))
 
ft
```

This is then used to estimate the power to detect various proportional changes over time (@fig-power).

```{r}
#| echo: false
#| warning: false
#| message: false
#| error: true
#| fig-cap: "Estimated power to detect proportional changes over time"
#| label: fig-power

# remove any missing values for resid.sd
residual.sd <- residual.sd[!is.na(residual.sd$sd),]

# estimate power at changes of 0 to .10/year with sample sizes of 5 to 20 years for each response
scenarios <- expand.grid(n.years=seq(5,20,1),
                         slope  =seq(0, .1, .02))
power.detect <- plyr::ddply(residual.sd, "Response", function(x, scenarios){
   
   power <- plyr::adply(scenarios,1, function(scenario,x){
      #browser()
      power <- slr.power.stroup(Trend=scenario$slope,
                                Xvalues=1:scenario$n.years,
                                Process.SD=x$sd,
                                Sampling.SD=0)
      power
   },x=x)
}, scenarios=scenarios)

ggplot(data=power.detect, aes(x=n.years, y=power.2s, color=as.factor(slope)))+
   ggtitle("Estimated power", subtitle="alpha=0.05")+
   geom_point()+
   geom_line()+
   facet_wrap(~Response, ncol=1)+
   ylab("Power")+xlab("Number of years in the study")+
   scale_color_discrete(name="Proportional\nyearly\nslope")+
   xlim(0,NA)+ylim(0,1)+
   geom_hline(yintercept=0.8)


```
 
The proportional yearly slope indicates the effect size of interest. For example, a value
of .02 would indicated a 2% change/year in the mean response.
 
In cases of high variability, the power is uniformly low 
to detect the yearly proportion change over time. 
In cases of low variability power is uniformly very high
to detect the yearly proportion change over time.



# Summary

Some caution is required to ensure that all transects are 
run the same number of times in a year. In this balanced design, 
it is straightforward to simply sum over all measurements of transect 
in a year and all transects in a year have the same number of visit. 
It is possible to modify the analysis is only some transects are visited on 
a particular date with an unequal number of visits to a transect in a year. 
A simple way to deal with unbalance would be to delete some of the observations, 
but better methods are available.


```{r}
#| echo: false
#| eval: false
#| include: false
##### if the lmer() function does not converge, you can repeat the analysis on the average of all the transect

# Compute the average total count for each transect so I can plot these over time
count.avg <- plyr::ddply(count.transect, c("STUDY_AREA_NAME","Year"), plyr::summarize,
                          count=mean(mean.calls, na.rm=TRUE))
count.avg

# Make a preliminary plot of average count by years

prelim.count.plot.avg <- ggplot(data=count.avg, aes(x=Year, y=log(count)))+
   ggtitle("log(Mean count) - averaged over all transects in a year")+
   ylab("log(Mean count) on the plots")+
   geom_point(position=position_dodge(width=.2))+
   geom_smooth(method="lm", se=FALSE)+
   scale_x_continuous(breaks=integer_breaks())+
   facet_wrap(~STUDY_AREA_NAME, ncol=1)
prelim.count.plot.avg 
ggsave(plot=prelim.count.plot.avg, 
       file=paste(file.prefix,'-count-plot-prelim-avg.png',sep=""),
       h=6, w=6, units="in",dpi=300)


# This is a simple regression analysis with Year as the trend variable 

count.fit.avg <-  lm(log(count) ~ Year, data=count.avg)
anova(count.fit.avg)
summary(count.fit.avg)

# Look at the residual plot 
diag.plot <- autoplot(count.fit.avg)  # residual and other diagnostic plots
show(diag.plot)
ggplot2::ggsave(#plot=diag.plot, # bug in ggmultiplot - just don't specify the plot object name
                file=paste(file.prefix,"-count-residual-avg-plot.png",sep=""),
                h=6, w=6, units="in", dpi=300)

# check for autocorrelation - look at the average residual over time
count.avg$resid <- log(count.avg$count) - predict(count.fit.avg, newdata=count.avg)
mean.resid <- plyr::ddply(count.avg, "Year", summarize, mean.resid=mean(resid))
resid.fit <- lm( mean.resid ~ 1, data=mean.resid)
dwres1 <- car::durbinWatsonTest(resid.fit)
dwres1
dwres2 <- lmtest::dwtest(resid.fit)
dwres2


# extract the slope
count.slopes.avg <- data.frame(
       STUDY_AREA_NAME =count.transect$STUDY_AREA_NAME[1],
       slope           = coef(count.fit.avg)["Year"],
       slope.se        = summary(count.fit.avg)$coefficients["Year","Pr(>|t|)"],
       p.value         = summary(count.fit.avg)$coefficients[row.names(summary(count.fit.avg)$coefficients)=="Year"  ,"Pr(>|t|)"], 
       r2              = summary(count.fit.avg)$r.squared, 
       stringsAsFactors=FALSE)
count.slopes.avg


# compute the fitted values from the model
count.fitted.avg <- data.frame(
                 STUDY_AREA_NAME=count.transect$STUDY_AREA_NAME[1],
                 Year=seq(min(count.avg$Year, na.rm=TRUE),max(count.avg$Year, na.rm=TRUE), .1),
                 stringsAsFactors=FALSE)
# because we fit on the log-scale, we need to antilog the predictions
count.fitted.avg$pred.mean <- exp(predict(count.fit.avg, newdata=count.fitted,type="response"))
head(count.fitted.avg)

# Plot with trend line 
count.plot.summary.avg <- ggplot2::ggplot(data=count.avg,
                                    aes(x=Year, y=count))+
   ggtitle("Total Species count")+
   ylab("Mean Total % count")+
   geom_point(size=3,position=position_dodge(w=0.2))+
   geom_line(data=count.fitted.avg, aes(x=Year,y=pred.mean))+
   facet_wrap(~STUDY_AREA_NAME, ncol=1, scales="free" )+
   scale_x_continuous(breaks=integer_breaks())+
   geom_text(data=count.slopes.avg, aes(x=min(count.avg$Year, na.rm=TRUE), y=max(count.avg$count, na.rm=TRUE)), 
             label=paste("Slope : ",round(count.slopes.avg$slope,2), 
                         " ( SE "  ,round(count.slopes.avg$slope.se,2),")",
                         " p :"    ,round(count.slopes.avg$p.value,3)),
                         hjust="left")
count.plot.summary.avg
ggsave(plot=count.plot.summary.avg, 
       file=paste(file.prefix,'-count-plot-summary-avg.png',sep=""),
       h=6, w=6, units="in", dpi=300)
```